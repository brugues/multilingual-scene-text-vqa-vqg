As the introduction has pointed out, the main objective of this project is to adapt current Visual Question Answering (VQA) models to  work on multiple languages and not just English.  To achieve the goals of this project, a series of VQA methods have been used, alongside Object Character Recognition (OCR) techniques and Multilingual embedding methods to encode the different languages into a single, common space.  In this section, we present a brief history on all the topics related to the project, from their beginnings to the latest and most important developments.  

In view of that, Section \ref{sota_vqa} focuses on VQA: Section \ref{sota_vqa_methods} explains some of the methodologies to solve this computer vision task while Section \ref{sota_vqa_datasets} focuses on the most important datsets that have been released. Next, \ref{sota_ocr} spotlights on Optical Character Recognition: Section \ref{sota_ocr_single_language} on single language OCR and Section \ref{sota_ocr_multiple_language} on multiple language OCR.  Following, Section \ref{sota_embeddings} explains the different approaches to obtain Multilingual embeddings: \ref{sota_word_embeddings} explains several techniques to obtain word embeddings while \ref{sota_sentence_embeddings} explains the approaches to obtain sentence embeddings. To finish up, Section \ref{sota_summary} wraps up the most important state-of-the-art research works seen in the following lines.


\subsection{Visual Question Answering}
\label{sota_vqa}
VQA is a computer vision task where a system is given a text-based question about an image,  and the algorithm must infer the answer based on information extracted on the image.  Questions can be arbitrary and they encompass many sub-problems in computer vision, such as:

\begin{itemize}
	\item Object recognition: “What is in the image?”
	\item Object detection: “Are there any cats in the image?”
	\item Attribute classification: “What color is the cat?”
	\item Scene classification: “Is it sunny?”
	\item Counting: “How many cats are in the image?”
\end{itemize}

\subsubsection{VQA Methods} 
\label{sota_vqa_methods}

Historically, VQA methods have focused on using text found on the image to answer the given question. 

\cite{vinyals2017pointer, DBLP:journals/corr/abs-1807-09956, singh2018pythia, Singh_2019_CVPR, 8978122, DBLP:journals/corr/abs-2006-00923, Hu_2020_CVPR}
% 2017, 2018, 2018, 2019, 2019, 2020, 2020

\cite{pennington2014glove} % Word embedding que utilitza algun paper.

\subsubsection{VQA Datasets}
\label{sota_vqa_datasets}

\cite{krishnavisualgenome, DBLP:journals/corr/VeitMNMB16, Johnson_2017_CVPR,Gurari_2018_CVPR, Biten_2019_ICCV, Singh_2019_CVPR}
% 2016, 2016, 2017, 2018, 2019, 2019


\begin{table}[b!]
\centering
\begin{tabular}{@{}llllll@{}}
\toprule
Visual Genome {[}X{]} & COCOText {[}X{]}                & Clevr {[}X{]}                      & VizWiz {[}X{]}                     & TextVQA {[}X{]} & {\color[HTML]{2E3436} ST-VQA {[}X{]}} \\ \midrule
Year                  & Year                            & {\color[HTML]{2E3436} Year}        & {\color[HTML]{2E3436} Year}        & Year            & Year                                  \\
x Images              & {\color[HTML]{2E3436} x Images} & {\color[HTML]{2E3436} x Images}    & {\color[HTML]{2E3436} x Images}    & x Images        & x Images                              \\
x Questions           & x Questions                     & {\color[HTML]{2E3436} x Questions} & {\color[HTML]{2E3436} x Questions} & x Questions     & x Questions                           \\ \midrule
                      &                                 &                                    &                                    &                 &                                      
\end{tabular}
\caption{Summary of the available VQA datasets and their characteristics}
\label{tbl:vqa_datasets}
\end{table}

\subsection{Optical Character Recognition}
\label{sota_ocr}

OCR is a computer vision task used to extract text –printed or handwritten–– from images. As we have seen from Section \ref{sota_vqa_methods}, it is one of the backbones on almost all VQA approaches.

\cite{wang2010word, mishra:hal-00818183}
% 2010,  2012
% Els que van aquí falta mirar si són single o multi language

\subsubsection{Single language OCR}
\label{sota_ocr_single_language}

\cite{}

\subsubsection{Multiple language OCR}
\label{sota_ocr_multiple_language}

\cite{borisyuk2018rosetta}
% 2018

\subsection{Multilingual embeddings}
\label{sota_embeddings}
Multilingual embeddings are used to represent words or sentences from multiple languages in a single vector space.  Unsupervised methods acquire the embeddings without the need of a cross-lingual supervision, which is a significant advantage over traditional, supervised methods that require some sort of supervision between languages \cite{chen2018unsupervised}.

\cite{TIEDEMANN12.463,mikolov2013exploiting, ferreira2016jointly,DBLP:journals/corr/abs-1810-04805}
% 2012, 2013,2016, 2018

\subsubsection{Word embeddings}
\label{sota_word_embeddings}

\cite{klementiev2012inducing, faruqui2014improving,hermann2014multilingual,lauly2014autoencoder, gouws2015bilbowa,levy2015improving, gouws2015bilbowa,luong2015bilingual, joulin2016bag,DBLP:journals/corr/SmithTHH17,conneau2017word,speer2017conceptnet,bojanowski2017enriching,chen2018unsupervised,heinzerling2018bpemb, jawanpuria2019learning}
% 2012, 2014, 2014,2014, 2015, 2015, 2015, 2015,2016,2017, 2017,2017, 2017, 2018, 2018, 2019

\subsubsection{Sentence embeddings}
\label{sota_sentence_embeddings}

\cite{le2014distributed, 10.1162/tacl_a_00288}
% 2014, 2019


\subsection{Summary}
\label{sota_summary}


